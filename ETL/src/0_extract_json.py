import csv 
import json
import requests
from bs4 import BeautifulSoup
from datetime import datetime

BASE_URL = "https://www.congreso.es:443/es/opendata/votaciones"
ROMAN_MAP = {10: "X", 11: "XI", 12: "XII", 13: "XIII", 14: "XIV", 15: "XV"}
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",
    "Accept-Language": "en-US,en;q=0.9",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Cache-Control": "max-age=0",
    "Referer": "https://www.congreso.es/",
    "DNT": "1"
}
BASE = "https://www.congreso.es"


def generate_main_url_from_json(legislatura_number):
    """
    Generates the main access URL for a legislature by reading the dates from its JSON file.

    Args:
        legislatura_number (int): Number of the legislature (10 to 15).

    Returns:
        list: List of URLs generated for all dates in the JSON file of the legislature.
    """
    JSON_FILE_PATH = f"input/votaciones_{legislatura_number}.json"
    
    # Validate the legislature number
    if legislatura_number not in ROMAN_MAP:
        raise ValueError("Legislature must be between 10 and 15.")
    
    # Read the JSON file and generate the URLs
    try:
        with open(JSON_FILE_PATH, "r") as file:
            dates = json.load(file)
    except FileNotFoundError:
        raise FileNotFoundError(f"JSON file not found: {JSON_FILE_PATH}")
    except json.JSONDecodeError:
        raise ValueError(f"Error reading or decoding the JSON file: {JSON_FILE_PATH}")
    
    urls = [f"{BASE_URL}?p_p_id=votaciones&p_p_lifecycle=0&p_p_state=normal&p_p_mode=view&targetLegislatura={ROMAN_MAP[legislatura_number]}&targetDate={datetime.strptime(str(date), '%Y%m%d').strftime('%d/%m/%Y')}" for date in dates]
    
    return urls

def save_json_href_to_csv(urls):
    """
    Saves the JSON links found in the generated URLs to a CSV file.

    Args:
        urls (list): List of URLs generated by generate_main_url_from_json.
    """
    csv_file_path = "downloads/json_href.csv"
    with open(csv_file_path, 'w', newline='') as csvfile:
        csvwriter = csv.writer(csvfile)
        csvwriter.writerow(['href'])
        for url in urls:
            response = requests.get(url, headers=HEADERS)
            soup = BeautifulSoup(response.text, 'html.parser')
            for link in soup.find_all('a'):
                href = link.get('href')
                if href and href.endswith('.json'):
                    csvwriter.writerow([href])
                    print(f"Saved: {href}")

def get_and_save_json_from_href(csv_file_path):
    """
    Retrieves and saves the JSON data from the links stored in the CSV file.

    Args:
        csv_file_path (str): Path of the CSV file containing the links.

    Returns:
        None
    """
    with open(csv_file_path, 'r') as csvfile:
        csvreader = csv.reader(csvfile)
        next(csvreader)  # Skip the header
        for row in csvreader:
            href = row[0]
            full_url = f"{BASE}{href}"
            response = requests.get(full_url, headers=HEADERS)
            json_data = response.json()
            with open(f"downloads/{href.split('/')[-1]}.json", 'w') as jsonfile:
                json.dump(json_data, jsonfile)
            print(f"Saved: {full_url}")

if __name__ == "__main__":
    all_urls = []
    for legislatura_number in range(10, 16):
        urls = generate_main_url_from_json(legislatura_number)
        all_urls.extend(urls)
    save_json_href_to_csv(all_urls)
    get_and_save_json_from_href("downloads/json_href.csv")